# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "ebe866dc-d917-446f-a2fa-ce8f67955344",
# META       "default_lakehouse_name": "Podcast_Lake",
# META       "default_lakehouse_workspace_id": "b3347730-e9a0-4182-b008-ef53a5c4d3d5",
# META       "known_lakehouses": [
# META         {
# META           "id": "ebe866dc-d917-446f-a2fa-ce8f67955344"
# META         }
# META       ]
# META     }
# META   }
# META }

# CELL ********************

load = spark.sql("""
    SELECT * 
    FROM Podcast_Lake.episodes 
    LIMIT 1000
    """
    )
display(load)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

def clean_data(load):
    # Drop columns: 'rss.channel.title', 'rss.channel.description' and 30 other columns
    load = load.drop('rss.channel.title', 'rss.channel.description', 'rss.channel.link', 'rss.channel.generator', 'rss.channel.lastBuildDate', 'rss.channel.atom:link.@href', 'rss.channel.atom:link.@rel', 'rss.channel.atom:link.@type', 'rss.channel.author', 'rss.channel.copyright', 'rss.channel.language', 'rss.channel.itunes:author', 'rss.channel.itunes:summary', 'rss.channel.itunes:type', 'rss.channel.itunes:owner.itunes:name', 'rss.channel.itunes:owner.itunes:email', 'rss.channel.itunes:explicit', 'rss.channel.itunes:category.@text', 'rss.channel.itunes:image.@href', 'rss.channel.item.title', 'rss.channel.item.description', 'rss.channel.item.guid.@isPermaLink', 'rss.channel.item.dc:creator', 'rss.channel.item.enclosure.@url', 'rss.channel.item.enclosure.@length', 'rss.channel.item.enclosure.@type', 'rss.channel.item.itunes:explicit', 'rss.channel.item.itunes:duration', 'rss.channel.item.itunes:image.@href', 'rss.channel.item.itunes:episodeType', 'rss.@version', 'rss.channel.item.itunes:season')
    
    # Rename columns 'rss.channel.item.link' to 'url_link'
    load = load.withColumnRenamed('rss.channel.item.link',           'url_link')
    load = load.withColumnRenamed('rss.channel.item.guid._value_',   'guid_id')
    load = load.withColumnRenamed('rss.channel.item.pubDate',        'publication_date')
    load = load.withColumnRenamed('rss.channel.item.itunes:summary', 'summary')
    load = load.withColumnRenamed('rss.channel.item.itunes:episode', 'episode_number')

    # Replace all instances of <p> and </p>
    load = load.withColumn('summary', F.regexp_replace('summary', "(?i)<p>", ""))
    load = load.withColumn('summary', F.regexp_replace('summary', "(?i)</p>", ""))

    return load

load_clean = clean_data(load)
display(load_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Custom date parser function without using prebuilt libraries
from pyspark.sql import functions as F
from pyspark.sql.types import StringType

def custom_date_parser(date_string):
    """
    Custom function to parse 'Thu, 03 Feb 2022 14:56:23 GMT' to '2022-02-03 14:56:23'
    without using prebuilt date libraries
    """
    if date_string is None:
        return None
    
    # Month mapping
    month_map = {
        'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',
        'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',
        'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'
    }
    
    try:
        # Split the string: "Thu, 03 Feb 2022 14:56:23 GMT"
        parts = date_string.strip().split()
        
        # Extract components
        day = parts[1]  # "03"
        month_abbr = parts[2]  # "Feb"
        year = parts[3]  # "2022"
        time_part = parts[4]  # "14:56:23"
        
        # Convert month abbreviation to number
        month = month_map.get(month_abbr, '01')
        
        # Format as YYYY-MM-DD HH:mm:ss
        formatted_date = f"{year}-{month}-{day} {time_part}"
        
        return formatted_date
    
    except (IndexError, KeyError, AttributeError):
        # Return original string if parsing fails
        return date_string

# Register the function as a UDF (User Defined Function) for PySpark
from pyspark.sql.functions import udf
custom_date_parser_udf = udf(custom_date_parser, StringType())

def parse_publication_date_custom(df):
    """
    Parse publication_date column using custom parser function
    
    Args:
        df: PySpark DataFrame with publication_date column
    
    Returns:
        PySpark DataFrame with parsed publication_date column
    """
    df_parsed = df.withColumn(
        'publication_date',
        custom_date_parser_udf(F.col('publication_date'))
    )
    
    return df_parsed

# Alternative: Using only PySpark built-in string functions (no UDF)
def parse_publication_date_spark_only(df):
    """
    Parse publication_date using only PySpark string functions
    Example: "Thu, 03 Feb 2022 14:56:23 GMT" -> "2022-02-03 14:56:23"
    """
    # Create month mapping using when/otherwise
    df_parsed = df.withColumn('temp_split', F.split(F.col('publication_date'), ' '))
    
    # Extract parts
    df_parsed = df_parsed.withColumn('day', F.col('temp_split')[1])
    df_parsed = df_parsed.withColumn('month_abbr', F.col('temp_split')[2])
    df_parsed = df_parsed.withColumn('year', F.col('temp_split')[3])
    df_parsed = df_parsed.withColumn('time_part', F.col('temp_split')[4])
    
    # Convert month abbreviation to number using when/otherwise
    df_parsed = df_parsed.withColumn('month',
        F.when(F.col('month_abbr') == 'Jan', '01')
        .when(F.col('month_abbr') == 'Feb', '02')
        .when(F.col('month_abbr') == 'Mar', '03')
        .when(F.col('month_abbr') == 'Apr', '04')
        .when(F.col('month_abbr') == 'May', '05')
        .when(F.col('month_abbr') == 'Jun', '06')
        .when(F.col('month_abbr') == 'Jul', '07')
        .when(F.col('month_abbr') == 'Aug', '08')
        .when(F.col('month_abbr') == 'Sep', '09')
        .when(F.col('month_abbr') == 'Oct', '10')
        .when(F.col('month_abbr') == 'Nov', '11')
        .when(F.col('month_abbr') == 'Dec', '12')
        .otherwise('01')
    )
    
    # Concatenate to form the final date string
    df_parsed = df_parsed.withColumn('publication_date',
        F.concat(
            F.col('year'), F.lit('-'),
            F.col('month'), F.lit('-'),
            F.col('day'), F.lit(' '),
            F.col('time_part')
        )
    )
    
    # Drop temporary columns
    df_parsed = df_parsed.drop('temp_split', 'day', 'month_abbr', 'year', 'time_part', 'month')
    
    return df_parsed

print("Custom date parsing functions created successfully!")
print("Use parse_publication_date_custom() for UDF approach")
print("Use parse_publication_date_spark_only() for pure PySpark approach")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

load_clean_2 = parse_publication_date_spark_only(load_clean)

display(load_clean_2.head(5))

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

load_clean_2.write.mode("overwrite").format("delta").saveAsTable('episode_list')

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
